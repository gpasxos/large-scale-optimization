{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN3JTRdI+jUw/GIgNrZTt1s",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gpasxos/large-scale-optimization/blob/main/ch06_newton_method.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Newton's Method with Backtracking Line Search\n",
        "\n",
        "Newton's method updates the iterate according to:\n",
        "$$x^{(k+1)} = x^{(k)} + \\eta^{(k)} \\Delta x_{\\text{nt}}$$\n",
        "\n",
        "where the **Newton step** is:\n",
        "$$\\Delta x_{\\text{nt}} = -[\\nabla^2 f(x^{(k)})]^{-1} \\nabla f(x^{(k)})$$\n",
        "\n",
        "### Newton Decrement\n",
        "\n",
        "The **Newton decrement** provides a stopping criterion:\n",
        "$$\\lambda(x) = \\sqrt{\\nabla f(x)^T [\\nabla^2 f(x)]^{-1} \\nabla f(x)}$$\n",
        "\n",
        "We stop when $\\lambda^2/2 \\leq \\epsilon$, since this approximates the function decrease from one Newton step.\n",
        "\n",
        "### Backtracking Line Search\n",
        "\n",
        "We use **Armijo backtracking**: start with $\\eta = 1$ and shrink by factor $\\beta$ until:\n",
        "$$f(x + \\eta \\Delta x_{\\text{nt}}) \\leq f(x) - \\alpha \\eta \\lambda^2$$\n",
        "\n",
        "## Demonstration: Ill-Conditioned Quadratic\n",
        "\n",
        "We test on a quadratic $f(x) = \\frac{1}{2} x^T Q x + b^T x$ where $Q$ has condition number $\\kappa = 1000$.\n",
        "\n",
        "**Why this is a challenging test case:**\n",
        "- High condition number severely slows gradient descent (rate $\\approx 1 - 1/\\kappa$)\n",
        "- Newton's affine invariance means $\\kappa$ doesn't affect its convergence\n",
        "\n",
        "## Key Takeaways\n",
        "\n",
        "| Method | Iterations | Per-iteration cost | Total cost |\n",
        "|--------|------------|-------------------|------------|\n",
        "| Newton | $O(\\log\\log(1/\\epsilon))$ | $O(n^3)$ | Low |\n",
        "| Gradient Descent | $O(\\kappa \\log(1/\\epsilon))$ | $O(n^2)$ | High when $\\kappa$ large |\n",
        "\n",
        "1. **Quadratic convergence**: Once near the optimum, Newton doubles correct digits each iteration\n",
        "2. **Affine invariance**: Newton's performance is independent of condition number $\\kappa$\n",
        "3. **Practical regime**: Prefer Newton when $n \\lesssim 10^4$ and high accuracy is needed"
      ],
      "metadata": {
        "id": "cRtbfjUmA16G"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fc2fs9ub_crR",
        "outputId": "6b5ab0e9-b9ad-4bbe-fb60-893027bb604b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "Newton's Method on Ill-Conditioned Quadratic\n",
            "Dimension: 20, Condition number: 1000\n",
            "============================================================\n",
            "Iter   0: f = -5.756522e-01, lambda = 1.36e+03, eta = 1.0000\n",
            "Converged at iteration 1: lambda^2/2 = 4.24e-25\n",
            "\n",
            "Final error: ||x - x*|| = 9.00e-13\n",
            "Final objective gap: f(x) - f* = -1.71e-14\n",
            "\n",
            "============================================================\n",
            "Gradient Descent for comparison\n",
            "============================================================\n",
            "GD converged in 1000 iterations\n",
            "Newton converged in 1 iterations\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from scipy.linalg import cho_factor, cho_solve\n",
        "def newton_method(f, grad_f, hess_f, x0, tol=1e-10, max_iter=100, alpha=0.25, beta=0.5, verbose=True):\n",
        "    \"\"\"\n",
        "    Newton's method with backtracking line search.\n",
        "    Parameters:\n",
        "        f: objective function\n",
        "        grad_f: gradient function\n",
        "        hess_f: Hessian function (returns n x n matrix)\n",
        "        x0: initial point\n",
        "        tol: tolerance for Newton decrement squared / 2\n",
        "        alpha: Armijo parameter (0 < alpha < 0.5)\n",
        "        beta: backtracking shrinkage factor (0 < beta < 1)\n",
        "        verbose: print progress\n",
        "\n",
        "    Returns:\n",
        "        x: optimal point\n",
        "        history: dict with convergence information\n",
        "    \"\"\"\n",
        "    x = np.array(x0, dtype=float)\n",
        "    n = len(x)\n",
        "\n",
        "    history = {\n",
        "        'f': [f(x)],\n",
        "        'newton_decrement': [],\n",
        "        'step_size': [],\n",
        "        'x': [x.copy()]\n",
        "    }\n",
        "\n",
        "    for k in range(max_iter):\n",
        "        # Compute gradient and Hessian\n",
        "        g = grad_f(x)\n",
        "        H = hess_f(x)\n",
        "\n",
        "        # Solve H @ dx = -g for Newton step using Cholesky factorization\n",
        "        try:\n",
        "            c, lower = cho_factor(H)\n",
        "            dx = cho_solve((c, lower), -g)\n",
        "        except np.linalg.LinAlgError:\n",
        "            print(\"Hessian not positive definite, adding regularization\")\n",
        "            H_reg = H + 1e-6 * np.eye(n)\n",
        "            c, lower = cho_factor(H_reg)\n",
        "            dx = cho_solve((c, lower), -g)\n",
        "\n",
        "        # Compute Newton decrement\n",
        "        lambda_sq = -g @ dx # = g^T H^{-1} g\n",
        "        newton_decrement = np.sqrt(lambda_sq)\n",
        "        history['newton_decrement'].append(newton_decrement)\n",
        "\n",
        "        # Check stopping criterion\n",
        "        if lambda_sq / 2 <= tol:\n",
        "            if verbose:\n",
        "                print(f\"Converged at iteration {k}: lambda^2/2 = {lambda_sq/2:.2e}\")\n",
        "            break\n",
        "\n",
        "        # Backtracking line search\n",
        "        eta = 1.0\n",
        "        f_x = f(x)\n",
        "        while f(x + eta * dx) > f_x - alpha * eta * lambda_sq:\n",
        "            eta *= beta\n",
        "            if eta < 1e-16:\n",
        "                print(\"Warning: step size too small\")\n",
        "                break\n",
        "\n",
        "        history['step_size'].append(eta)\n",
        "\n",
        "        # Update\n",
        "        x = x + eta * dx\n",
        "        history['f'].append(f(x))\n",
        "        history['x'].append(x.copy())\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"Iter {k:3d}: f = {f(x):.6e}, lambda = {newton_decrement:.2e}, eta = {eta:.4f}\")\n",
        "\n",
        "    return x, history\n",
        "\n",
        "def demo_newton():\n",
        "    \"\"\"Demonstrate Newton's method on an ill-conditioned quadratic.\"\"\"\n",
        "    np.random.seed(42)\n",
        "    n = 20\n",
        "\n",
        "    # Create ill-conditioned quadratic: f(x) = 0.5 * x^T Q x + b^T x\n",
        "    kappa = 1000 # Condition number\n",
        "    eigenvalues = np.linspace(1, kappa, n)\n",
        "    Q_diag = np.diag(eigenvalues)\n",
        "\n",
        "    # Random rotation\n",
        "    U, _ = np.linalg.qr(np.random.randn(n, n))\n",
        "    Q = U @ Q_diag @ U.T\n",
        "    b = np.random.randn(n)\n",
        "\n",
        "    # Optimal solution\n",
        "    x_star = np.linalg.solve(Q, -b)\n",
        "    f_star = 0.5 * x_star @ Q @ x_star + b @ x_star\n",
        "\n",
        "    f = lambda x: 0.5 * x @ Q @ x + b @ x\n",
        "    grad_f = lambda x: Q @ x + b\n",
        "    hess_f = lambda x: Q\n",
        "\n",
        "    # Initial point\n",
        "    x0 = np.random.randn(n) * 10\n",
        "\n",
        "    print(\"=\"*60)\n",
        "    print(\"Newton's Method on Ill-Conditioned Quadratic\")\n",
        "    print(f\"Dimension: {n}, Condition number: {kappa}\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    x_newton, history = newton_method(f, grad_f, hess_f, x0, verbose=True)\n",
        "\n",
        "    print(f\"\\nFinal error: ||x - x*|| = {np.linalg.norm(x_newton - x_star):.2e}\")\n",
        "    print(f\"Final objective gap: f(x) - f* = {f(x_newton) - f_star:.2e}\")\n",
        "\n",
        "    # Compare with gradient descent\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"Gradient Descent for comparison\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    x_gd = x0.copy()\n",
        "    eta_gd = 1.0 / kappa # Step size = 1/L\n",
        "    gd_errors = [np.linalg.norm(x_gd - x_star)]\n",
        "\n",
        "    for k in range(1000):\n",
        "        x_gd = x_gd - eta_gd * grad_f(x_gd)\n",
        "        gd_errors.append(np.linalg.norm(x_gd - x_star))\n",
        "        if gd_errors[-1] < 1e-10:\n",
        "            break\n",
        "\n",
        "    print(f\"GD converged in {len(gd_errors)-1} iterations\")\n",
        "    print(f\"Newton converged in {len(history['f'])-1} iterations\")\n",
        "\n",
        "    return history, gd_errors\n",
        "\n",
        "# Run demonstration\n",
        "history, gd_errors = demo_newton()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6pYrh1U_AbhJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}