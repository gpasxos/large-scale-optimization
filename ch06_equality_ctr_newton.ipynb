{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPVdM984fvCS4tPrf06VTuU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gpasxos/large-scale-optimization/blob/main/ch06_equality_ctr_newton.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Newton's Method for Equality-Constrained Optimization\n",
        "\n",
        "We solve problems of the form:\n",
        "$$\\min_x f(x) \\quad \\text{subject to} \\quad Ax = b$$\n",
        "\n",
        "## KKT System for Newton Step\n",
        "\n",
        "At each iteration, we find the Newton step $\\Delta x$ and dual variables $\\nu$ by solving the KKT system:\n",
        "\n",
        "$$\\begin{bmatrix} \\nabla^2 f(x) & A^T \\\\ A & 0 \\end{bmatrix} \\begin{bmatrix} \\Delta x \\\\ \\nu \\end{bmatrix} = \\begin{bmatrix} -\\nabla f(x) \\\\ b - Ax \\end{bmatrix}$$\n",
        "\n",
        "**Key properties:**\n",
        "- If $Ax^{(k)} = b$ (feasible), then $A\\Delta x = 0$, so $x^{(k+1)} = x^{(k)} + \\eta \\Delta x$ remains feasible for any $\\eta$\n",
        "- For infeasible start, the right-hand side $b - Ax$ drives the iterates toward feasibility\n",
        "- The dual variables $\\nu$ are the Lagrange multipliers at convergence\n",
        "\n",
        "## Stopping Criterion\n",
        "\n",
        "We check both:\n",
        "1. **Newton decrement**: $\\lambda^2/2 \\leq \\epsilon$ (optimality)\n",
        "2. **Constraint violation**: $\\|Ax - b\\| < \\epsilon$ (feasibility)\n",
        "\n",
        "## Demo Problem\n",
        "\n",
        "We minimize a quadratic $f(x) = \\frac{1}{2}x^T Q x + c^T x$ subject to $Ax = b$ with:\n",
        "- $n = 10$ variables, $p = 3$ equality constraints\n",
        "- Random positive definite $Q$\n",
        "- Starting from an infeasible point $x_0 = 0$"
      ],
      "metadata": {
        "id": "NMA09mEZF00L"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "od30rocYBu9R",
        "outputId": "80f3aff2-3c81-44f7-f28d-e48ca4c93e0f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "Newton for Equality-Constrained Optimization\n",
            "============================================================\n",
            "Warning: initial point not feasible, using infeasible-start Newton\n",
            "Iter 0: f = -9.773477e-01, lambda = 1.83e+00, ||Ax-b|| = 1.78e-15, eta = 1.0000\n",
            "Converged at iteration 1\n",
            "\n",
            "Optimal x (first 5): [ 1.12463372  1.85995114 -1.43299828  0.92893075  0.8391533 ]\n",
            "Optimal multipliers: [-0.75215757  0.53110276  0.08493105]\n",
            "Final constraint violation: 1.78e-15\n",
            "KKT residual ||grad f + A^T nu||: 2.77e-15\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from scipy.linalg import lu_factor, lu_solve\n",
        "\n",
        "def newton_equality_constrained(f, grad_f, hess_f, A, b, x0, tol=1e-10, max_iter=100, alpha=0.25, beta=0.5):\n",
        "    \"\"\"\n",
        "    Newton's method for equality-constrained optimization.\n",
        "\n",
        "    min f(x) s.t. Ax = b\n",
        "    \"\"\"\n",
        "    x = np.array(x0, dtype=float)\n",
        "    n = len(x)\n",
        "    p = len(b)\n",
        "\n",
        "    # Check initial feasibility\n",
        "    residual = A @ x - b\n",
        "    if np.linalg.norm(residual) > 1e-10:\n",
        "        print(\"Warning: initial point not feasible, using infeasible-start Newton\")\n",
        "\n",
        "    history = {'f': [f(x)], 'newton_decrement': [], 'constraint_violation': [np.linalg.norm(residual)]}\n",
        "\n",
        "    for k in range(max_iter):\n",
        "        g = grad_f(x)\n",
        "        H = hess_f(x)\n",
        "        residual = A @ x - b\n",
        "\n",
        "        # Form and solve KKT system\n",
        "        # [H A^T] [dx] [-g ]\n",
        "        # [A 0 ] [nu] = [b - Ax ]\n",
        "        KKT = np.block([\n",
        "            [H, A.T],\n",
        "            [A, np.zeros((p, p))]\n",
        "        ])\n",
        "        rhs = np.concatenate([-g, -residual])\n",
        "\n",
        "        try:\n",
        "            lu, piv = lu_factor(KKT)\n",
        "            sol = lu_solve((lu, piv), rhs)\n",
        "        except np.linalg.LinAlgError:\n",
        "            print(\"KKT system singular\")\n",
        "            break\n",
        "\n",
        "        dx = sol[:n]\n",
        "        nu = sol[n:]\n",
        "\n",
        "        # Newton decrement\n",
        "        lambda_sq = -g @ dx\n",
        "        if lambda_sq < 0:\n",
        "            lambda_sq = abs(lambda_sq) # Numerical issue\n",
        "        newton_decrement = np.sqrt(lambda_sq)\n",
        "        history['newton_decrement'].append(newton_decrement)\n",
        "\n",
        "        # Check convergence\n",
        "        if lambda_sq / 2 <= tol and np.linalg.norm(residual) < tol:\n",
        "            print(f\"Converged at iteration {k}\")\n",
        "            break\n",
        "\n",
        "        # Backtracking line search\n",
        "        eta = 1.0\n",
        "        f_x = f(x)\n",
        "\n",
        "        # Merit function: f(x) + penalty * ||Ax - b||\n",
        "        penalty = 1.0\n",
        "        merit = lambda z: f(z) + penalty * np.linalg.norm(A @ z - b)\n",
        "        merit_x = merit(x)\n",
        "\n",
        "        while merit(x + eta * dx) > merit_x - alpha * eta * lambda_sq:\n",
        "            eta *= beta\n",
        "            if eta < 1e-16:\n",
        "                break\n",
        "\n",
        "        x = x + eta * dx\n",
        "        history['f'].append(f(x))\n",
        "        history['constraint_violation'].append(np.linalg.norm(A @ x - b))\n",
        "\n",
        "        print(f\"Iter {k}: f = {f(x):.6e}, lambda = {newton_decrement:.2e}, \"f\"||Ax-b|| = {np.linalg.norm(A @ x - b):.2e}, eta = {eta:.4f}\")\n",
        "\n",
        "    return x, nu, history\n",
        "\n",
        "def demo_equality_constrained():\n",
        "    \"\"\"Demo: minimize quadratic subject to linear equality constraints.\"\"\"\n",
        "    np.random.seed(42)\n",
        "    n, p = 10, 3\n",
        "\n",
        "    # Objective: f(x) = 0.5 * x^T Q x + c^T x\n",
        "    Q = np.eye(n) + 0.5 * np.random.randn(n, n)\n",
        "    Q = Q @ Q.T # Make positive definite\n",
        "    c = np.random.randn(n)\n",
        "\n",
        "    f = lambda x: 0.5 * x @ Q @ x + c @ x\n",
        "    grad_f = lambda x: Q @ x + c\n",
        "    hess_f = lambda x: Q\n",
        "\n",
        "    # Constraints: Ax = b\n",
        "    A = np.random.randn(p, n)\n",
        "    x_feas = np.random.randn(n)\n",
        "    b = A @ x_feas # Ensure feasibility is possible\n",
        "\n",
        "    # Initial point (may not be feasible)\n",
        "    x0 = np.zeros(n)\n",
        "\n",
        "    print(\"=\"*60)\n",
        "    print(\"Newton for Equality-Constrained Optimization\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    x_opt, nu_opt, history = newton_equality_constrained(\n",
        "        f, grad_f, hess_f, A, b, x0\n",
        "    )\n",
        "\n",
        "    print(f\"\\nOptimal x (first 5): {x_opt[:5]}\")\n",
        "    print(f\"Optimal multipliers: {nu_opt}\")\n",
        "    print(f\"Final constraint violation: {np.linalg.norm(A @ x_opt - b):.2e}\")\n",
        "\n",
        "    # Verify KKT conditions\n",
        "    kkt_residual = grad_f(x_opt) + A.T @ nu_opt\n",
        "    print(f\"KKT residual ||grad f + A^T nu||: {np.linalg.norm(kkt_residual):.2e}\")\n",
        "\n",
        "demo_equality_constrained()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "V5wgrpcZFY58"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}